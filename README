

1/ Install and run ZeroLeptonRun2:
==================================

The instructions for installing and compiling are in ZeroLeptonRun2/INSTALL 

The cafe config file is in ZeroLeptonRun2/config/zerolepton.config, you can
copy it and modify it but for testing it is easier to just overwrite some 
configurations from the command line like:

cafe ZeroLeptonRun2/config/zerolepton.config Events: 100 Input: root://eosatlas.cern.ch//eos/atlas/user/j/jpoveda/mc14_13TeV.204422.Herwigpp_UEEE4_CTEQ6L1_Tt_T800_L1.merge.AOD.e3064_s1982_s2008_r5787_r5853_tid01604167_00/AOD.01604167._000005.pool.root.1

This run a simple job with no systematics. A preliminary implementation of
systematics that adds a tree for every systematics uses the 
zerolepton_with_syst.config  config file.




2/ Extract dataset informations from AMI:
=========================================
localSetupPyAMI
ZeroLeptonRun2/python/makeDatasetList.py --help

For example, 
ZeroLeptonRun2/python/makeDatasetList.py --baseline

will make the list of mc14_13TeV corresponding to the baseline samples.




3/ Running jobs on the grid:
============================
#to reduce the size of the tarball it's better to clear libraries etc 
rc clean
localSetupPandaClient
ZeroLeptonRun2/python/launch_jobs_on_grid.py --help

For example, with baseline.ds a text file with the list of dataset extracted with makeDatasetList.py:
ZeroLeptonRun2/python/launch_jobs_on_grid.py --prunopts='--nGBPerJob=10 --excludedSite=ANALY_UIO,SIGNET,LUNARC ' --prefix='user.lduflot.' --suffix='_test2' --tmpDir=/scratch/duflot  --inDSfile baseline.ds

If the inDS contains the string data11/data12, the job will be configured to 
read real data. For MC, if the inDS still has the AMI tags, the script can
figure out if ATLFAST was used. On the other hand, it knows about a limited 
pattern of signal datasets, to be safe use the --signal option.




4/ Creating an updated cross-section database:
==============================================
This step is necessary if you made a production that may not be represented by
the cross-section file in ZeroLeptonRun2/data, e.g. when the number of events
you ran on may not be the same.

Use the getMCInfo.py tools (slightly updated version oof the Run1 tool)
to build the extended cross-section database from the un-merged small
ntuples. The tool will extract the number of events processed and the 
total sum of MC weights from the Counter and write an extended 
cross-section file that includes these informations.

For example, one can list all downloaded small tuples in a file input.list
(the file path should allow to extract the dataset IDs) an do

ZeroLeptonRun2/python/getMCInfo.py --input input.list

this will create MCBackgroundDB.dat (and .tex) in the local directory.
You need to have the code compiled as it uses C++ code from SUSYTools.




5/ Merging small ntuples for ZeroLeptonHistFitter:
==================================================
The filter/merge/update step is done by FilterUpdateMerge.py:
ZeroLeptonRun2/python/FilterUpdateMerge.py --help  

For example:
ZeroLeptonRun2/python/FilterUpdateMerge.py --doBackground --input_mc=small.list --doNormWeight  --dbName updatedDB.dat

Where the --dbName option points to an updated cross-section database built 
with the previous step (needed as the previous steps adds the number of events
processed and corresponding sum weights.

The file path of input files for MC should allow to retreive the dataset ID,
 so this is currently not possible for files in rucio.

You need to have the code compiled as it uses C++ code.
